{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074273c9",
   "metadata": {},
   "source": [
    "# Subject independent Combined MS-DCN Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef7f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "import mne\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from braindecode.models import Deep4Net\n",
    "from braindecode.classifier import EEGClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# change directory go into Notebooks folder\n",
    "if os.path.basename(os.getcwd()) != 'Notebooks':\n",
    "    if os.path.basename(os.getcwd()) == 'lib':\n",
    "        os.chdir(os.path.join(os.getcwd(), '..', 'Notebooks'))\n",
    "    else:\n",
    "        os.chdir(os.path.join(os.getcwd(), 'Notebooks'))\n",
    "else:\n",
    "    # if already in Notebooks folder, do nothing\n",
    "    pass\n",
    "\n",
    "from lib import my_functions as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c1029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_models in results: 50\n",
      "N_models in ms_results: 50\n"
     ]
    }
   ],
   "source": [
    "type_of_subject = 'independent'  # 'independent' or 'adaptive'\n",
    "\n",
    "data_path = '../Data/'\n",
    "output_path = '../Output/ica_rest_all/'\n",
    "do_all = False\n",
    "n_subjects = 50\n",
    "subject_list = list(range(n_subjects))\n",
    "all_data, all_y = mf.load_all_data(subjects_list=None, do_all=do_all)\n",
    "\n",
    "ms_timeseries_path = os.path.join(output_path, 'ms_timeseries.npy')\n",
    "with open(ms_timeseries_path, 'rb') as f:\n",
    "    finals_ls = pickle.load(f)\n",
    "\n",
    "\n",
    "results = np.load(os.path.join(output_path, 'results_ica_rest_all.npy'), allow_pickle=True).item()\n",
    "ms_results = np.load(os.path.join(output_path, 'ms_results_ica_rest_all.npy'), allow_pickle=True).item()\n",
    "print(f'N_models in results: {len(results[\"models\"])}')\n",
    "print(f'N_models in ms_results: {len(ms_results[\"models\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e355b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Extracts features from pre-trained models by removing the final classification layer\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        \n",
    "        # Get the feature extractor (everything except the final classifier)\n",
    "        if hasattr(pretrained_model, 'module'):\n",
    "            # If it's wrapped in EEGClassifier, get the underlying network\n",
    "            self.backbone = pretrained_model.module\n",
    "        else:\n",
    "            self.backbone = pretrained_model\n",
    "            \n",
    "        # Remove the final classification layer\n",
    "        # For Deep4Net, the classifier is typically the last layer\n",
    "        if hasattr(self.backbone, 'final_layer'):\n",
    "            # If there's a specific final layer attribute\n",
    "            modules = list(self.backbone.children())[:-1]\n",
    "        elif hasattr(self.backbone, 'classifier'):\n",
    "            # If there's a classifier attribute\n",
    "            modules = [module for name, module in self.backbone.named_children() \n",
    "                      if name != 'classifier']\n",
    "        else:\n",
    "            # Remove the last linear/conv layer\n",
    "            modules = list(self.backbone.children())[:-1]\n",
    "            \n",
    "        self.feature_extractor = nn.Sequential(*modules)\n",
    "        \n",
    "        # Freeze the feature extractor\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extractor(x)\n",
    "            if len(features.shape) > 2:\n",
    "                # max pooling across time dimension\n",
    "                # features = F.adaptive_max_pool1d(features.flatten(1, 2), 1).squeeze(-1)\n",
    "                # globale average pooling\n",
    "                features = F.adaptive_avg_pool1d(features.flatten(1, 2), 1).squeeze(-1)\n",
    "\n",
    "\n",
    "            return features\n",
    "\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    \"\"\"Classifier that takes features from multiple modalities\"\"\"\n",
    "    \n",
    "    def __init__(self, raw_feature_dim, ms_feature_dim, n_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.raw_feature_dim = raw_feature_dim\n",
    "        self.ms_feature_dim = ms_feature_dim\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Feature fusion layer\n",
    "        total_features = raw_feature_dim + ms_feature_dim\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(total_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, raw_features, ms_features):\n",
    "        # Concatenate features from both modalities\n",
    "        combined_features = torch.cat([raw_features, ms_features], dim=1)\n",
    "        return self.classifier(combined_features)\n",
    "\n",
    "def extract_features_from_single_model(model, data, device):\n",
    "    \"\"\"Extract features from a single model for one subject's data\"\"\"\n",
    "    \n",
    "    # Convert to tensor if needed\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        x = torch.tensor(data, dtype=torch.float32)\n",
    "    else:\n",
    "        x = data.clone()\n",
    "    \n",
    "    print(f\"  Original data shape: {x.shape}\")\n",
    "    \n",
    "    # Handle different data formats based on the expected input\n",
    "    if len(x.shape) == 4:\n",
    "        # Could be (n_trials, 1, n_channels, timepoints) or (n_trials, n_channels, 1, timepoints)\n",
    "        if x.shape[1] == 1:  # (n_trials, 1, n_channels, timepoints) - raw EEG format\n",
    "            x = x.squeeze(1)  # Remove singleton dimension -> (n_trials, n_channels, timepoints)\n",
    "            print(f\"  Raw EEG format detected, shape after squeeze: {x.shape}\")\n",
    "        elif x.shape[2] == 1:  # (n_trials, n_channels, 1, timepoints)\n",
    "            x = x.squeeze(2)  # Remove singleton dimension -> (n_trials, n_channels, timepoints)\n",
    "            print(f\"  Format with singleton at dim 2, shape after squeeze: {x.shape}\")\n",
    "        else:\n",
    "            print(f\"  4D format without singleton, keeping as is: {x.shape}\")\n",
    "    elif len(x.shape) == 3:\n",
    "        # Could be (n_trials, n_channels, timepoints) - microstate format\n",
    "        print(f\"  3D format (likely microstate), shape: {x.shape}\")\n",
    "    else:\n",
    "        print(f\"  Unexpected shape: {x.shape}\")\n",
    "    \n",
    "    # Create feature extractor\n",
    "    feature_extractor = FeatureExtractor(model).to(device)\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # Extract features\n",
    "    features_list = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch_x = x[i:i+batch_size].to(device)\n",
    "            batch_features = feature_extractor(batch_x)\n",
    "            features_list.append(batch_features.cpu())\n",
    "    \n",
    "    subject_features = torch.cat(features_list, dim=0)\n",
    "    print(f\"  Extracted features shape: {subject_features.shape}\")\n",
    "    \n",
    "    return subject_features\n",
    "\n",
    "def train_single_multimodal_classifier(raw_features, ms_features, labels, n_classes, \n",
    "                                     subject_id, test_size=0.2, val_size=0.25, \n",
    "                                     device='cuda', num_epochs=100, lr=0.001):\n",
    "    \"\"\"Train a classifier on combined features for a single subject\"\"\"\n",
    "    \n",
    "    print(f\"\\nTraining multimodal classifier for Subject {subject_id}\")\n",
    "    print(f\"Raw features shape: {raw_features.shape}\")\n",
    "    print(f\"MS features shape: {ms_features.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Unique labels: {torch.unique(labels)}\")\n",
    "    \n",
    "    # set_seed(42 + subject_id)  # Different seed for each subject\n",
    "    set_seed(42)  # Use a fixed seed for reproducibility\n",
    "    \n",
    "    # Check if we have enough samples for splitting\n",
    "    if len(labels) < 10:\n",
    "        print(f\"Warning: Only {len(labels)} samples for subject {subject_id}. Using simple train/test split.\")\n",
    "        test_size = 0.3\n",
    "        val_size = 0.0  # No validation set for small datasets\n",
    "    \n",
    "    # Split data\n",
    "    indices = np.arange(len(labels))\n",
    "    \n",
    "    if val_size > 0:\n",
    "        train_val_idx, test_idx = train_test_split(\n",
    "            indices, test_size=test_size, random_state=42, \n",
    "            stratify=labels.numpy() if len(np.unique(labels.numpy())) > 1 else None\n",
    "        )\n",
    "        \n",
    "        train_idx, val_idx = train_test_split(\n",
    "            train_val_idx, test_size=val_size, random_state=42,\n",
    "            stratify=labels[train_val_idx].numpy() if len(np.unique(labels[train_val_idx].numpy())) > 1 else None\n",
    "        )\n",
    "    else:\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            indices, test_size=test_size, random_state=42,\n",
    "            stratify=labels.numpy() if len(np.unique(labels.numpy())) > 1 else None\n",
    "        )\n",
    "        val_idx = []\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(\n",
    "        raw_features[train_idx], ms_features[train_idx], labels[train_idx]\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        raw_features[test_idx], ms_features[test_idx], labels[test_idx]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = min(32, len(train_idx) // 2)  # Adjust batch size for small datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    if val_idx.any():\n",
    "        val_dataset = TensorDataset(\n",
    "            raw_features[val_idx], ms_features[val_idx], labels[val_idx]\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        val_loader = None\n",
    "    \n",
    "    # Initialize model\n",
    "    raw_feature_dim = raw_features.shape[1]\n",
    "    ms_feature_dim = ms_features.shape[1]\n",
    "    \n",
    "    model = MultiModalClassifier(\n",
    "        raw_feature_dim, ms_feature_dim, n_classes\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    if val_loader:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=10\n",
    "        )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for raw_batch, ms_batch, label_batch in train_loader:\n",
    "            raw_batch = raw_batch.to(device)\n",
    "            ms_batch = ms_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(raw_batch, ms_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * raw_batch.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            train_correct += (preds == label_batch).sum().item()\n",
    "            train_total += label_batch.size(0)\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total * 100\n",
    "        \n",
    "        # Validation (if available)\n",
    "        val_acc = 0\n",
    "        val_loss = 0\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for raw_batch, ms_batch, label_batch in val_loader:\n",
    "                    raw_batch = raw_batch.to(device)\n",
    "                    ms_batch = ms_batch.to(device)\n",
    "                    label_batch = label_batch.to(device)\n",
    "                    \n",
    "                    outputs = model(raw_batch, ms_batch)\n",
    "                    loss = criterion(outputs, label_batch)\n",
    "                    \n",
    "                    val_loss += loss.item() * raw_batch.size(0)\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    val_correct += (preds == label_batch).sum().item()\n",
    "                    val_total += label_batch.size(0)\n",
    "            \n",
    "            val_loss /= val_total\n",
    "            val_acc = val_correct / val_total * 100\n",
    "            scheduler.step(val_acc)\n",
    "        else:\n",
    "            val_acc = train_acc  # Use training accuracy as proxy\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"  Epoch {epoch}/{num_epochs}: \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "                  f\"Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Load best model and test\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    \n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for raw_batch, ms_batch, label_batch in test_loader:\n",
    "            raw_batch = raw_batch.to(device)\n",
    "            ms_batch = ms_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            \n",
    "            outputs = model(raw_batch, ms_batch)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            test_correct += (preds == label_batch).sum().item()\n",
    "            test_total += label_batch.size(0)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "    \n",
    "    test_acc = test_correct / test_total * 100\n",
    "    \n",
    "    print(f\"  Subject {subject_id} Results:\")\n",
    "    print(f\"    Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"    Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'test_accuracy': test_acc,\n",
    "        'best_val_accuracy': best_val_acc,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accs,\n",
    "        'val_accuracies': val_accs,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels,\n",
    "        'subject_id': subject_id\n",
    "    }\n",
    "\n",
    "def run_subject_specific_multimodal_pipeline(results, ms_results, all_data, finals_ls, all_y, \n",
    "                                             n_subjects=50, device='cuda'):\n",
    "    \"\"\"\n",
    "    Leave-one-subject-out (LOSO) training:\n",
    "      - 1 subject = test\n",
    "      - 4 randomly sampled validation subjects (fixed seed)\n",
    "      - 45 remaining subjects for training\n",
    "    Uses the same output format as the original function.\n",
    "    \"\"\"\n",
    "    print(f\"Starting subject-specific multimodal pipeline for {n_subjects} subjects...\")\n",
    "\n",
    "    \n",
    "    output_file = os.path.join(output_path, f'{type_of_subject}_comb_results_ica_rest_all.npy')\n",
    "\n",
    "    for subject_id in range(n_subjects):\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"Loading existing results for Subject {subject_id} from {output_file}\")\n",
    "            all_subject_results = np.load(output_file, allow_pickle=True).tolist()\n",
    "        else:\n",
    "            all_subject_results = []\n",
    "        if len(all_subject_results) > subject_id:\n",
    "            print(f\"Skipping Subject {subject_id}, results already exist.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing Subject {subject_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        set_seed(42)  # Ensure reproducible splits\n",
    "\n",
    "        # --- LOSO Subject Splits ---\n",
    "        other_ids = list(range(50))\n",
    "        other_ids.remove(subject_id)\n",
    "        val_ids = random.sample(other_ids, 4)\n",
    "        train_ids = [i for i in other_ids if i not in val_ids]\n",
    "\n",
    "        print(f\"Train IDs: {train_ids}\")\n",
    "        print(f\"Val IDs:   {val_ids}\")\n",
    "        print(f\"Test ID:   {subject_id}\")\n",
    "\n",
    "        # --- Extract Features ---\n",
    "        def extract_group_features(ids, model_dict, data_source):\n",
    "            features, labels = [], []\n",
    "            for i in ids:\n",
    "                feats = extract_features_from_single_model(model_dict['models'][i], data_source[i], device)\n",
    "                features.append(feats)\n",
    "                labels.append(torch.tensor(all_y[i], dtype=torch.long))\n",
    "            return torch.cat(features), torch.cat(labels)\n",
    "\n",
    "        raw_train_feats, y_train = extract_group_features(train_ids, results, all_data)\n",
    "        ms_train_feats, _ = extract_group_features(train_ids, ms_results, finals_ls)\n",
    "\n",
    "        raw_val_feats, y_val = extract_group_features(val_ids, results, all_data)\n",
    "        ms_val_feats, _ = extract_group_features(val_ids, ms_results, finals_ls)\n",
    "\n",
    "        raw_features = torch.cat([raw_train_feats, raw_val_feats], dim=0)\n",
    "        ms_features = torch.cat([ms_train_feats, ms_val_feats], dim=0)\n",
    "        labels = torch.cat([y_train, y_val], dim=0)\n",
    "\n",
    "        val_size_ratio = len(y_val) / len(labels)\n",
    "\n",
    "        # --- Train Classifier ---\n",
    "        subject_results = train_single_multimodal_classifier(\n",
    "            raw_features, ms_features, labels,\n",
    "            n_classes=len(torch.unique(labels)),\n",
    "            subject_id=subject_id,\n",
    "            test_size=val_size_ratio,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # --- Test on held-out subject ---\n",
    "        raw_test_feats = extract_features_from_single_model(results['models'][subject_id], all_data[subject_id], device)\n",
    "        ms_test_feats = extract_features_from_single_model(ms_results['models'][subject_id], finals_ls[subject_id], device)\n",
    "        y_test = torch.tensor(all_y[subject_id], dtype=torch.long)\n",
    "\n",
    "        model = subject_results['model'].to(device)\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(y_test), 32):\n",
    "                out = model(raw_test_feats[i:i+32].to(device), ms_test_feats[i:i+32].to(device))\n",
    "                preds.extend(out.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        test_acc = accuracy_score(y_test.numpy(), preds) * 100\n",
    "        print(f\"✅ Subject {subject_id} Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "        subject_results.update({\n",
    "            'test_accuracy': test_acc,\n",
    "            'true_labels': y_test.numpy(),\n",
    "            'predictions': preds\n",
    "        })\n",
    "\n",
    "        all_subject_results.append(subject_results)\n",
    "        print(f\"Saving results for Subject {subject_id}...\")\n",
    "        np.save(output_file, all_subject_results, allow_pickle=True)\n",
    "\n",
    "\n",
    "    # --- Summary ---\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY OF ALL SUBJECTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    test_accuracies = [res['test_accuracy'] for res in all_subject_results]\n",
    "    val_accuracies = [res['best_val_accuracy'] for res in all_subject_results]\n",
    "\n",
    "    print(f\"Test Accuracies:\")\n",
    "    for i, acc in enumerate(test_accuracies):\n",
    "        print(f\"  Subject {i}: {acc:.2f}%\")\n",
    "\n",
    "    print(f\"\\nAverage Test Accuracy: {np.mean(test_accuracies):.2f}% ± {np.std(test_accuracies):.2f}%\")\n",
    "    print(f\"Average Validation Accuracy: {np.mean(val_accuracies):.2f}% ± {np.std(val_accuracies):.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'subject_results': all_subject_results,\n",
    "        'summary': {\n",
    "            'test_accuracies': test_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'mean_test_acc': np.mean(test_accuracies),\n",
    "            'std_test_acc': np.std(test_accuracies),\n",
    "            'mean_val_acc': np.mean(val_accuracies),\n",
    "            'std_val_acc': np.std(val_accuracies)\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de1981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting subject-specific multimodal pipeline for 50 subjects...\n",
      "Loading existing results for Subject 0 from ../Output/ica_rest_all/independent_comb_results_ica_rest_all.npy\n",
      "Skipping Subject 0, results already exist.\n",
      "Loading existing results for Subject 1 from ../Output/ica_rest_all/independent_comb_results_ica_rest_all.npy\n",
      "\n",
      "============================================================\n",
      "Processing Subject 1\n",
      "============================================================\n",
      "Train IDs: [0, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "Val IDs:   [30, 27, 10, 9]\n",
      "Test ID:   1\n",
      "  Original data shape: torch.Size([400, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([400, 61, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([400, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([400, 61, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([280, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([280, 61, 1000])\n",
      "  Extracted features shape: torch.Size([280, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([400, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([400, 61, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([400, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([400, 61, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([400, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([400, 61, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([400, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([400, 61, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([300, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([300, 61, 1000])\n",
      "  Extracted features shape: torch.Size([300, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([320, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([320, 61, 1000])\n",
      "  Extracted features shape: torch.Size([320, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([300, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([300, 61, 1000])\n",
      "  Extracted features shape: torch.Size([300, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([340, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([340, 61, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([380, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([380, 61, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([400, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([400, 7, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([400, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([400, 7, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([280, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([280, 7, 1000])\n",
      "  Extracted features shape: torch.Size([280, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([400, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([400, 7, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([400, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([400, 7, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([400, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([400, 7, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([400, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([400, 7, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([300, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([300, 7, 1000])\n",
      "  Extracted features shape: torch.Size([300, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([320, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([320, 7, 1000])\n",
      "  Extracted features shape: torch.Size([320, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([300, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([300, 7, 1000])\n",
      "  Extracted features shape: torch.Size([300, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([340, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([340, 7, 1000])\n",
      "  Extracted features shape: torch.Size([340, 1400])\n",
      "  Original data shape: torch.Size([380, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([380, 7, 1000])\n",
      "  Extracted features shape: torch.Size([380, 1400])\n",
      "  Original data shape: torch.Size([300, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([300, 61, 1000])\n",
      "  Extracted features shape: torch.Size([300, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([400, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([400, 61, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([360, 1, 61, 1000])\n",
      "  Raw EEG format detected, shape after squeeze: torch.Size([360, 61, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([300, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([300, 7, 1000])\n",
      "  Extracted features shape: torch.Size([300, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "  Original data shape: torch.Size([400, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([400, 7, 1000])\n",
      "  Extracted features shape: torch.Size([400, 1400])\n",
      "  Original data shape: torch.Size([360, 7, 1000])\n",
      "  3D format (likely microstate), shape: torch.Size([360, 7, 1000])\n",
      "  Extracted features shape: torch.Size([360, 1400])\n",
      "\n",
      "Training multimodal classifier for Subject 1\n",
      "Raw features shape: torch.Size([17580, 1400])\n",
      "MS features shape: torch.Size([17580, 1400])\n",
      "Labels shape: torch.Size([17580])\n",
      "Unique labels: tensor([0, 1, 2])\n",
      "  Epoch 0/100: Train Loss: 0.6091, Train Acc: 71.92%, Val Acc: 84.13%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Run the subject-specific multimodal pipeline for all subjects\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m multimodal_results = \u001b[43mrun_subject_specific_multimodal_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinals_ls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_subjects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mms_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Save the results\u001b[39;00m\n\u001b[32m     10\u001b[39m output_file = os.path.join(output_path, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_of_subject\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_multimodal_results_ica_rest_all.npy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 386\u001b[39m, in \u001b[36mrun_subject_specific_multimodal_pipeline\u001b[39m\u001b[34m(results, ms_results, all_data, finals_ls, all_y, n_subjects, device)\u001b[39m\n\u001b[32m    383\u001b[39m val_size_ratio = \u001b[38;5;28mlen\u001b[39m(y_val) / \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[32m    385\u001b[39m \u001b[38;5;66;03m# --- Train Classifier ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m subject_results = \u001b[43mtrain_single_multimodal_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mms_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_size_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;66;03m# --- Test on held-out subject ---\u001b[39;00m\n\u001b[32m    395\u001b[39m raw_test_feats = extract_features_from_single_model(results[\u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m][subject_id], all_data[subject_id], device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mtrain_single_multimodal_classifier\u001b[39m\u001b[34m(raw_features, ms_features, labels, n_classes, subject_id, test_size, val_size, device, num_epochs, lr)\u001b[39m\n\u001b[32m    225\u001b[39m outputs = model(raw_batch, ms_batch)\n\u001b[32m    226\u001b[39m loss = criterion(outputs, label_batch)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m optimizer.step()\n\u001b[32m    230\u001b[39m train_loss += loss.item() * raw_batch.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josan\\Documents\\EPFL\\These\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josan\\Documents\\EPFL\\These\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\josan\\Documents\\EPFL\\These\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run the subject-specific multimodal pipeline for all subjects\n",
    "multimodal_results = run_subject_specific_multimodal_pipeline(\n",
    "    results, ms_results, all_data, finals_ls, all_y, \n",
    "    n_subjects=len(ms_results['models']), device=device\n",
    ")\n",
    "# Save the results\n",
    "output_file = os.path.join(output_path, f'{type_of_subject}_multimodal_results_ica_rest_all.npy')\n",
    "np.save(output_file, multimodal_results, allow_pickle=True)\n",
    "\n",
    "print(f\"\\nMultimodal pipeline completed!\")\n",
    "print(f\"Average performance across 50 subjects: {multimodal_results['summary']['mean_test_acc']:.2f}%\")\n",
    "\n",
    "# Access individual subject results\n",
    "for i, subject_result in enumerate(multimodal_results['subject_results']):\n",
    "    print(f\"Subject {i}: {subject_result['test_accuracy']:.2f}% test accuracy\")\n",
    "    # Access the trained model: subject_result['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38adcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the multimodal results\n",
    "output_path = '../Output/ica_rest_all/'\n",
    "output_file = os.path.join(output_path, f'{type_of_subject}_multimodal_results_ica_rest_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c28cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = [res['test_accuracy'] for res in multimodal_results['subject_results']]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(range(len(accuracy_list)), accuracy_list, marker='o', linestyle='-')\n",
    "plt.title(f'Subject {type_of_subject} Test Accuracies for Each Subject')\n",
    "plt.xlabel('Subject ID')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.xticks(range(len(accuracy_list)), [f'S{i}' for i in range(len(accuracy_list))], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, f'{type_of_subject}_multimodal_test_accuracies.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(results['test_accuracies'], marker='x', linestyle='--', label='DeepConvNet Raw EEG', color='orange')\n",
    "plt.plot(ms_results['test_accuracies'], marker='s', linestyle=':', label='DeepConvNet Microstate', color='green')\n",
    "plt.plot(accuracy_list, marker='o', linestyle='-', label='Combined', color='blue')\n",
    "plt.xlabel('Subject ID')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.xticks(range(50), [f'S{i}' for i in range(50)], rotation=45)\n",
    "plt.title(f'Subject {type_of_subject} Test Accuracies for Each Subject')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, f'{type_of_subject}_multimodal_vs_individual_test_accuracies.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b014fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(data=[results['test_accuracies'], ms_results['test_accuracies'], accuracy_list],\n",
    "               palette=['orange', 'green', 'blue'], cut=0)\n",
    "plt.xticks([0, 1, 2], ['DeepConvNet Raw EEG', 'DeepConvNet Microstate', 'Combined'])\n",
    "plt.title(f'Subject {type_of_subject} Distribution of Test Accuracies')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.xlabel('Model Type')\n",
    "plt.ylim(0, 100)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_path, f'{type_of_subject}_test_accuracy_distribution.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c60a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
